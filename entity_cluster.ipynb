{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster semantically related sentences from a document.\n",
    "`Extract relevant sentences using existing methods. The RST(Rhetorical Structer Theory) method extracts nuclei within sentences using a discourse parser. The second method extracts sentences that are considered on-topic.\n",
    "In both cases, our system is then run on those sentences that were considered more relevant, with improved results as we shall see in the next subsections`\n",
    "\n",
    "`characteristics such as similarity between sentences, grammatical relationship and number of entities are enough to determine the type of rhetorical relation of most data sets`\n",
    "\n",
    "`Semantic orientation (SO) is a measure of subjectivity and opinion in text. It usually captures an evaluative factor (positive or negative) and potency (degree to which the document in question is positive or negative) towards a subject topic, person or idea (Osgood et al.,1957). When used in the analysis of public opinion, such as the automated interpretation of online product reviews, semantic orientation can be extremely helpful in marketing, measures of popularity and success, and compiling reviews.`\n",
    "\n",
    "`several   cohesive   devices   helpful   for   the   semanticinterpretation of the whole text: coreference relations (various expressions referring to the same entity),discourse   connectives,   lexical   relations   such   as   synonymy,   hypernymy,   hyponymy,   meronymy,   andthematic progressions.Among these cohesive devices, coreference relations are expressed via anaphoricchains or reference chains . Anaphoric chains consist of two elements:the anaphor (an expression semantically related to a discourse entity already introduced in the text) and itsantecedent (the referred or related entity). The anaphor and its antecedent might be related by varioussemantic relations such as  referential  identity or meronymy. Reference chains contain at  least threereferring expressions, related to the same entity.`\n",
    "\n",
    "* A discourse-aware neural network-based text model for document-level text classification (https://www.semanticscholar.org/paper/A-discourse-aware-neural-network-based-text-model-Lee-Han/e3731eec8b06bf0a85fe2f597b61d29525312f77) (https://journals.sagepub.com/doi/abs/10.1177/0165551517743644?journalCode=jisb)\n",
    "* https://hpi.de/fileadmin/user_upload/fachgebiete/plattner/teaching/NaturalLanguageProcessing/NLP2016/NLP07_DiscourseAnalysis.pdf\n",
    "* https://www.researchgate.net/publication/228784687_Extracting_Sentiment_as_a_Function_of_Discourse_Structure_and_Topicality\n",
    "* Exploiting Discourse Relations between Sentences for Text Clustering (https://www.aclweb.org/anthology/W12-4702)\n",
    "* APPLICATION OF RHETORICAL RELATIONS BETWEEN SENTENCES TO CLUSTER-BASED TEXT SUMMARIZATION(https://airccj.org/CSCP/vol5/csit53307.pdf)\n",
    "* http://www.emnlp2015.org/proceedings/EMNLP/pdf/EMNLP263.pdf\n",
    "* https://github.com/parry2403/R2N2/blob/master/RSTParser/tree.py\n",
    "* https://github.com/irit-melodi/attelo\n",
    "* Entropy and Graph Based Modelling of DocumentCoherence using Discourse Entities: An Application to IR (https://arxiv.org/pdf/1507.08234v1.pdf)\n",
    "* Discourse Coherence in the Wild: A Dataset, Evaluation and Methods(https://arxiv.org/pdf/1805.04993.pdf)\n",
    "* Using Rhetorical Structure Theory and Entity Grids to Automatically Evaluate Local Coherence in Texts(https://link.springer.com/chapter/10.1007/978-3-319-09761-9_26)\n",
    "* Sentence Ordering and Coherence Modeling using Recurrent Neural Networks(https://arxiv.org/pdf/1611.02654.pdf)\n",
    "* opinion mining and sentiment analysis (http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected entities : dict_keys(['ADP', 'Investor Relations', 'Elena', 'the Investor Relations', 'ADP.com', 'IR', 'EPS', 'National Accounts Division', 'Accounts Division', 'Our International Division', 'PEO', 'Employer Services', 'ES', 'Dealer Services', 'General Motors', 'Saturn'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndoc = nlp(u\"This is a sentence. This is another sentence.\")\\nfor sent in doc.sents:\\n    print(sent.text)\\n    print(get_all_entity(sent.text))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get input text\n",
    "with open(\"datasets/Quarterly_earnings/F1Q10 Earnings Call.txt\",'r') as f:\n",
    "    text = f.read()\n",
    "text = text.replace(\"\\n\",\"\")\n",
    "\n",
    "def get_all_entity(x):\n",
    "    doc = nlp(x)\n",
    "    return [(ent.text,ent.label_) for ent in doc.ents if ent.label_ in ['ORG','GPE','LOC']]\n",
    "\n",
    "#get_all_entity(data)\n",
    "data_doc = nlp(text)\n",
    "#sent_ent = []\n",
    "cluster_sents = {}\n",
    "for sent in data_doc.sents:\n",
    "    #print(sent.text)\n",
    "    if len(sent.text) > 2:\n",
    "        #print(get_all_entity(sent.text))\n",
    "        ent_list = get_all_entity(sent.text)\n",
    "        for i in ent_list:\n",
    "            #cluster_sents[i[0]] = \n",
    "            cluster_sents.setdefault(i[0], []).append(sent.text)\n",
    "        #sent_ent.append((sent.text,ent_list))\n",
    "\n",
    "#print(cluster_sents)\n",
    "print(\"selected entities : {}\".format(cluster_sents.keys()))\n",
    "'''\n",
    "doc = nlp(u\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print(get_all_entity(sent.text))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiments using trained model. (and model can be trained using sent_classify_gensim_logit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embd_org size :  400000\n",
      "[ 2.18620125e-01  2.81472875e-01 -1.25106875e-01  1.50036250e-01\n",
      "  5.96343750e-01  3.22677875e-01 -3.13277013e-01 -1.56962500e-01\n",
      " -4.68037529e-01 -3.47117500e-01  5.51527500e-02  1.98861250e-01\n",
      " -3.15731125e-01 -6.99631250e-03  4.22275063e-01  3.30072250e-01\n",
      "  3.31215625e-01  5.57588750e-02 -4.45320375e-02  1.90551375e-01\n",
      "  6.86348750e-02  2.29917500e-01  6.15846250e-02 -1.14000000e-03\n",
      "  3.55171250e-01 -1.95211250e+00 -1.84775000e-01 -1.63584875e-01\n",
      " -3.37210500e-01  3.41669500e-01  3.28406250e+00  1.02216000e-01\n",
      " -4.85905000e-01 -8.60532500e-02  7.66383913e-02  7.14194125e-02\n",
      "  8.65075000e-02  1.53622000e-01 -6.14226250e-02  2.87821250e-02\n",
      " -3.40749125e-01  5.98435000e-01 -2.10346500e-01 -3.86917000e-01\n",
      "  5.53227500e-02  3.06330000e-01 -7.61394513e-01  1.39012500e-02\n",
      "  4.83082750e-01 -2.62835125e-01]\n"
     ]
    }
   ],
   "source": [
    "# finetune glove vector\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "glove_filename = \"pretrained-word-vectors/glove.6B.50d.txt\"\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "# Load `original_embedding`\n",
    "word_embd_org = glove2dict(glove_filename)\n",
    "print(\"word_embd_org size : \",len(word_embd_org))\n",
    "\n",
    "word2idx = {}\n",
    "embedding_matrix = []\n",
    "\n",
    "for i, key in enumerate(word_embd_org.keys()):\n",
    "    word, vector = key , word_embd_org.get(key)\n",
    "    word2idx[word] = i\n",
    "    embedding = np.array(vector).astype(float)\n",
    "    embedding_matrix.append(embedding)\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "word2idx = word2idx\n",
    "\n",
    "# sentence embedding\n",
    "in_text = \"who is the prime minister of india ?\"\n",
    "indices = [word2idx[w] for w in in_text.split() if w in word2idx]\n",
    "embeddings = embedding_matrix[indices]\n",
    "avg_embedding = np.mean(embeddings, axis=0)\n",
    "print(avg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "#filename = 'model/sentiment_model.sav'\n",
    "filename = 'model/sentiment_model_using_glove_minmax_svm.sav'\n",
    "clf_pred = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def min_max(in_text):\n",
    "    indices = [word2idx[w] for w in in_text.split() if w in word2idx]\n",
    "    embeddings = embedding_matrix[indices]\n",
    "    return embeddings.min(axis=0) + embeddings.max(axis=0)\n",
    "\n",
    "def predict(clf_pred, sent):\n",
    "    test = [min_max(sent)]\n",
    "    y_pred = clf_pred.predict(test)\n",
    "    return y_pred\n",
    "\n",
    "def predict_proba(clf_pred, sent):\n",
    "    test = [min_max(sent)]\n",
    "    y_pred = clf_pred.predict_proba(test)\n",
    "    return y_pred\n",
    "\n",
    "output = predict(clf_pred, \"We believe STLD has the most near-term upside potential from exposure to favorable near-term pricing dynamics.\")\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity ## sentiments ## max_class['negative','positive','neutral']\n",
      "ADP ## [[0.07130599 0.4553116  0.47338241]\n",
      " [0.01529541 0.60963819 0.3750664 ]\n",
      " [0.01529541 0.60963819 0.3750664 ]\n",
      " [0.21533797 0.74255982 0.04210221]\n",
      " [0.21250799 0.74533054 0.04216148]\n",
      " [0.20833367 0.74869758 0.04296875]\n",
      " [0.21574492 0.74171182 0.04254325]] ## [0.1362602  0.66469825 0.19904156]\n",
      "Investor Relations ## [[0.11954878 0.40618049 0.47427072]] ## [0.11954878 0.40618049 0.47427072]\n",
      "Elena ## [[0.01529541 0.60963819 0.3750664 ]] ## [0.01529541 0.60963819 0.3750664 ]\n",
      "the Investor Relations ## [[0.21000571 0.74747244 0.04252185]] ## [0.21000571 0.74747244 0.04252185]\n",
      "IR ## [[0.21670141 0.7412266  0.04207199]] ## [0.21670141 0.7412266  0.04207199]\n",
      "EPS ## [[0.21533797 0.74255982 0.04210221]] ## [0.21533797 0.74255982 0.04210221]\n",
      "National Accounts Division ## [[0.56156664 0.39802907 0.04040429]] ## [0.56156664 0.39802907 0.04040429]\n",
      "Accounts Division ## [[0.08078165 0.39464689 0.52457146]] ## [0.08078165 0.39464689 0.52457146]\n",
      "Our International Division ## [[0.06444315 0.81091245 0.12464441]] ## [0.06444315 0.81091245 0.12464441]\n",
      "PEO ## [[0.01453893 0.67989054 0.30557053]] ## [0.01453893 0.67989054 0.30557053]\n",
      "Employer Services ## [[0.52657396 0.43583885 0.03758719]] ## [0.52657396 0.43583885 0.03758719]\n",
      "ES ## [[0.25967205 0.6990797  0.04124825]\n",
      " [0.72374498 0.14546252 0.1307925 ]] ## [0.49170851 0.42227111 0.08602037]\n",
      "Dealer Services ## [[0.72374498 0.14546252 0.1307925 ]\n",
      " [0.2508431  0.70771353 0.04144337]\n",
      " [0.20413666 0.75245494 0.04340841]\n",
      " [0.23636682 0.72184281 0.04179037]] ## [0.35377289 0.58186845 0.06435866]\n",
      "General Motors ## [[0.2112189  0.74622284 0.04255826]] ## [0.2112189  0.74622284 0.04255826]\n",
      "Saturn ## [[0.21574492 0.74171182 0.04254325]] ## [0.21574492 0.74171182 0.04254325]\n"
     ]
    }
   ],
   "source": [
    "# iter over each entity cluster and get probability\n",
    "print(\"entity ## sentiments ## max_class['negative','positive','neutral']\")\n",
    "label_dict = {0:'negative',1:'positive',2:'neutral'}\n",
    "for each_entity in cluster_sents.keys():\n",
    "    tmp = []\n",
    "    for each_sent in cluster_sents[each_entity]:\n",
    "        if len(each_sent.split()) > 1:\n",
    "            #print(each_sent)\n",
    "            tmp.extend(predict_proba(clf_pred, each_sent))\n",
    "    \n",
    "    if len(tmp) > 0:\n",
    "        #print(\"{} ## {} ## {}\".format(each_entity,tmp, max(tmp,key=tmp.count)))\n",
    "        tmp = np.array(tmp)\n",
    "        tmp_avg = tmp.mean(axis=0)\n",
    "        #print(\"{} ## {} ## {}\".format(each_entity,tmp, tmp_avg))\n",
    "        print(\"entity : {}\".format(each_entity))\n",
    "        for ind in label_dict.keys():\n",
    "            print(\"{} : {}\".format(label_dict[ind],tmp_avg[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let me leave ES and talk for a moment about Dealer Services.',\n",
       " 'As Chris will tell you in a few minutes, Dealer Services did record an intangible asset impairment charge in the quarter due to this expected closure of the Saturn dealerships.',\n",
       " 'we had previously estimated that the impact to Dealer Services for industry wide dealership closings over the next 12 to 18 months was at the low end of our original $50 to $75 million estimate.',\n",
       " 'I also want to point out that despite the continued tough automotive market Dealer Services is continuing to do very well on the competitive front.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data contained by entity.\n",
    "cluster_sents['Dealer Services']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for Subject in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "class SubjectTrigramTagger(object):\n",
    "\n",
    "    \"\"\" Creates an instance of NLTKs TrigramTagger with a backoff\n",
    "    tagger of a bigram tagger a unigram tagger and a default tagger that sets\n",
    "    all words to nouns (NN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "\n",
    "        \"\"\"\n",
    "        train_sents: trained sentences which have already been tagged.\n",
    "                Currently using Brown, conll2000, and TreeBank corpuses\n",
    "        \"\"\"\n",
    "\n",
    "        t0 = DefaultTagger('NN')\n",
    "        t1 = UnigramTagger(train_sents, backoff=t0)\n",
    "        t2 = BigramTagger(train_sents, backoff=t1)\n",
    "        self.tagger = TrigramTagger(train_sents, backoff=t2)\n",
    "\n",
    "    def tag(self, tokens):\n",
    "        return self.tagger.tag(tokens)\n",
    "\n",
    "\n",
    "train_sents = nltk.corpus.brown.tagged_sents()\n",
    "train_sents += nltk.corpus.conll2000.tagged_sents()\n",
    "train_sents += nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "# Create instance of SubjectTrigramTagger\n",
    "trigram_tagger = SubjectTrigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'dealer services']\n",
      "Let me leave ES and talk for a moment about Dealer Services.\n",
      "subject :  es\n",
      "{'subject': 'es', 'action': 'talk', 'object': 'moment', 'phrase': [('ES', 'NN'), ('and', 'CC'), ('talk', 'VB'), ('for', 'IN'), ('a', 'AT'), ('moment', 'NN')]}\n",
      "\n",
      "\n",
      "['chris', 'dealer services']\n",
      "As Chris will tell you in a few minutes, Dealer Services did record an intangible asset impairment charge in the quarter due to this expected closure of the Saturn dealerships.\n",
      "subject :  chris\n",
      "{'subject': 'chris', 'action': 'tell', 'object': 'minutes', 'phrase': [('Chris', 'NP'), ('will', 'MD'), ('tell', 'VB'), ('you', 'PPO'), ('in', 'IN'), ('a', 'AT'), ('few', 'AP'), ('minutes', 'NNS')]}\n",
      "\n",
      "\n",
      "['dealer services']\n",
      "we had previously estimated that the impact to Dealer Services for industry wide dealership closings over the next 12 to 18 months was at the low end of our original $50 to $75 million estimate.\n",
      "subject :  dealer services\n",
      "\n",
      "\n",
      "['dealer services']\n",
      "I also want to point out that despite the continued tough automotive market Dealer Services is continuing to do very well on the competitive front.\n",
      "subject :  dealer services\n",
      "{'subject': 'dealer services', 'action': 'continuing', 'object': 'front', 'phrase': [('dealer services', 'NN'), ('is', 'BEZ'), ('continuing', 'VBG'), ('to', 'TO'), ('do', 'DO'), ('very', 'QL'), ('well', 'RB'), ('on', 'IN'), ('the', 'AT'), ('competitive', 'JJ'), ('front', 'NN')]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from trigram_tagger import SubjectTrigramTagger\n",
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Noun Part of Speech Tags used by NLTK\n",
    "# More can be found here\n",
    "# http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
    "NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def download_document(url):\n",
    "    \"\"\"Downloads document using BeautifulSoup, extracts the subject and all\n",
    "    text stored in paragraph tags\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    title = soup.find('title').get_text()\n",
    "    document = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return document\n",
    "\n",
    "def clean_document(document):\n",
    "    \"\"\"Remove enronious characters. Extra whitespace and stop words\"\"\"\n",
    "    document = re.sub('[^A-Za-z .-]+', ' ', document)\n",
    "    document = ' '.join(document.split())\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    return document\n",
    "\n",
    "def tokenize_sentences(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def get_entities(document):\n",
    "    \"\"\"Returns Named Entities using NLTK Chunking\"\"\"\n",
    "    entities = []\n",
    "    sentences = tokenize_sentences(document)\n",
    "\n",
    "    # Part of Speech Tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                entities.append(' '.join([c[0] for c in chunk]).lower())\n",
    "    return entities\n",
    "\n",
    "def word_freq_dist(document):\n",
    "    \"\"\"Returns a word count frequency distribution\"\"\"\n",
    "    words = nltk.tokenize.word_tokenize(document)\n",
    "    words = [word.lower() for word in words if word not in stop]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist\n",
    "\n",
    "def extract_subject(document):\n",
    "    # Get most frequent Nouns\n",
    "    fdist = word_freq_dist(document)\n",
    "    most_freq_nouns = [w for w, c in fdist.most_common(10)\n",
    "                       if nltk.pos_tag([w])[0][1] in NOUNS]\n",
    "\n",
    "    # Get Top 10 entities\n",
    "    entities = get_entities(document)\n",
    "    top_10_entities = [w for w, c in nltk.FreqDist(entities).most_common(10)]\n",
    "\n",
    "    # Get the subject noun by looking at the intersection of top 10 entities\n",
    "    # and most frequent nouns. It takes the first element in the list\n",
    "    subject_nouns = [entity for entity in top_10_entities\n",
    "                    if entity.split()[0] in most_freq_nouns]\n",
    "    print(subject_nouns)\n",
    "    return subject_nouns[0]\n",
    "\n",
    "def trained_tagger(existing=False):\n",
    "    \"\"\"Returns a trained trigram tagger\n",
    "    existing : set to True if already trained tagger has been pickled\n",
    "    \"\"\"\n",
    "    if existing:\n",
    "        trigram_tagger = pickle.load(open('trained_tagger.pkl', 'rb'))\n",
    "        return trigram_tagger\n",
    "\n",
    "    # Aggregate trained sentences for N-Gram Taggers\n",
    "    train_sents = nltk.corpus.brown.tagged_sents()\n",
    "    train_sents += nltk.corpus.conll2000.tagged_sents()\n",
    "    train_sents += nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "    # Create instance of SubjectTrigramTagger and persist instance of it\n",
    "    trigram_tagger = SubjectTrigramTagger(train_sents)\n",
    "    pickle.dump(trigram_tagger, open('trained_tagger.pkl', 'wb'))\n",
    "\n",
    "    return trigram_tagger\n",
    "\n",
    "def tag_sentences(subject, document, trigram_tagger):\n",
    "    \"\"\"Returns tagged sentences using POS tagging\"\"\"\n",
    "    #trigram_tagger = trained_tagger(existing=True)\n",
    "\n",
    "    # Tokenize Sentences and words\n",
    "    sentences = tokenize_sentences(document)\n",
    "    merge_multi_word_subject(sentences, subject)\n",
    "\n",
    "    # Filter out sentences where subject is not present\n",
    "    sentences = [sentence for sentence in sentences if subject in\n",
    "                [word.lower() for word in sentence]]\n",
    "\n",
    "    # Tag each sentence\n",
    "    tagged_sents = [trigram_tagger.tag(sent) for sent in sentences]\n",
    "    return tagged_sents\n",
    "\n",
    "def merge_multi_word_subject(sentences, subject):\n",
    "    \"\"\"Merges multi word subjects into one single token\n",
    "    ex. [('steve', 'NN', ('jobs', 'NN')] -> [('steve jobs', 'NN')]\n",
    "    \"\"\"\n",
    "    if len(subject.split()) == 1:\n",
    "        return sentences\n",
    "    subject_lst = subject.split()\n",
    "    sentences_lower = [[word.lower() for word in sentence]\n",
    "                        for sentence in sentences]\n",
    "    for i, sent in enumerate(sentences_lower):\n",
    "        if subject_lst[0] in sent:\n",
    "            for j, token in enumerate(sent):\n",
    "                start = subject_lst[0] == token\n",
    "                exists = subject_lst == sent[j:j+len(subject_lst)]\n",
    "                if start and exists:\n",
    "                    del sentences[i][j+1:j+len(subject_lst)]\n",
    "                    sentences[i][j] = subject\n",
    "    return sentences\n",
    "\n",
    "def get_svo(sentence, subject):\n",
    "    \"\"\"Returns a dictionary containing:\n",
    "    subject : the subject determined earlier\n",
    "    action : the action verb of particular related to the subject\n",
    "    object : the object the action is referring to\n",
    "    phrase : list of token, tag pairs for that lie within the indexes of\n",
    "                the variables above\n",
    "    \"\"\"\n",
    "    subject_idx = next((i for i, v in enumerate(sentence)\n",
    "                    if v[0].lower() == subject), None)\n",
    "    data = {'subject': subject}\n",
    "    for i in range(subject_idx, len(sentence)):\n",
    "        found_action = False\n",
    "        for j, (token, tag) in enumerate(sentence[i+1:]):\n",
    "            if tag in VERBS:\n",
    "                data['action'] = token\n",
    "                found_action = True\n",
    "            if tag in NOUNS and found_action == True:\n",
    "                data['object'] = token\n",
    "                data['phrase'] = sentence[i: i+j+2]\n",
    "                return data\n",
    "    return {}\n",
    "\n",
    "xx = ['Let me leave ES and talk for a moment about Dealer Services.',\n",
    " 'As Chris will tell you in a few minutes, Dealer Services did record an intangible asset impairment charge in the quarter due to this expected closure of the Saturn dealerships.',\n",
    " 'we had previously estimated that the impact to Dealer Services for industry wide dealership closings over the next 12 to 18 months was at the low end of our original $50 to $75 million estimate.',\n",
    " 'I also want to point out that despite the continued tough automotive market Dealer Services is continuing to do very well on the competitive front.']\n",
    "for sub in xx:\n",
    "    subject = extract_subject(sub)\n",
    "    print(sub)\n",
    "    print(\"subject : \",subject)\n",
    "    \n",
    "    tagged_sents = tag_sentences(subject, sub, trigram_tagger)\n",
    "\n",
    "    svos = [get_svo(sentence, subject)\n",
    "                        for sentence in tagged_sents]\n",
    "    for svo in svos:\n",
    "        if svo:\n",
    "            print(svo)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discourse Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/howto/discourse.html\n",
    "import nltk\n",
    "\n",
    "dt = nltk.DiscourseTester(['a boxer walks', 'every boxer chases a girl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-stage Discourse Parser (recommend : run as separate rest service.)\n",
    "#! git clone https://github.com/yizhongw/StageDP.git\n",
    "\n",
    "* use python 3.5\n",
    "* preprocess (python src/preprocess.py --data_dir ../data/test2/ --corenlp_dir stanford-corenlp-full-2017-06-09/)\n",
    "* pred (python main.py --eval --eval_dir ../data/samples/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPLP\n",
    "* git clone https://github.com/jiyfeng/DPLP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/jiyfeng/DPLP.git\n",
    "#!python DPLP/rstparser.py ./StageDP/data/test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A PDTB-Styled End-to-End Discourse Parser\n",
    "https://github.com/WING-NUS/pdtb-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir('pdtb_parser')\n",
    "#! java -jar parser.jar examples/custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/parry2403/R2N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/irit-melodi/attelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://bitbucket.org/chloebt/discourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coreference resolution\n",
    "\n",
    "* https://www.rangakrish.com/index.php/2019/02/03/coreference-resolution-using-spacy/\n",
    "* https://www.rangakrish.com/index.php/2019/02/10/coreference-resolution-in-stanford-corenlp/\n",
    "* https://github.com/smartschat/cort\n",
    "* https://corpling.uis.georgetown.edu/xrenner/doc/using.html#importing-as-a-module\n",
    "* https://github.com/huggingface/neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[My sister: [My sister, She], a dog: [a dog, him]]\n"
     ]
    }
   ],
   "source": [
    "#! export CORENLP_HOME='stanford-corenlp-full-2017-06-09/'\n",
    "#! pip install stanford-corenlp\n",
    "#!pip install neuralcoref\n",
    "#import logging;\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "doc = nlp(u'My sister has a dog. She loves him.')\n",
    "\n",
    "print(doc._.has_coref)\n",
    "print(doc._.coref_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[that report: [that report, this report], the labor market: [the labor market, the labor market], people: [people, their], Manpower: [Manpower, they], Marriott International: [Marriott International, They], the road: [the road, the road], the strength: [the strength, it, It], us: [us, we]]\n",
      "True\n",
      "[that report: [that report, this report], the labor market: [the labor market, the labor market], people: [people, their], Manpower: [Manpower, they]]\n",
      "True\n",
      "[Marriott International: [Marriott International, They], the road: [the road, the road], the strength: [the strength, it, It], us: [us, we]]\n",
      "True\n",
      "[the strength: [the strength, it, It], us: [us, we]]\n",
      "True\n",
      "[Marriott International: [Marriott International, They]]\n"
     ]
    }
   ],
   "source": [
    "#subprocess.call(['./test.sh'])subprocess.call(['./test.sh'])subprocess.call(['./test.sh'])subprocess.call(['./test.sh'])subprocess.call(['./test.sh'])subprocess.call(['./test.sh'])# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "doc = nlp(u'''Let's start out for number one the first company that you're looking forward to and what are you looking for in that report?\n",
    "The first one is Manpower, which is a huge, temporary staffing organization. In this report, I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market. One of the big unanswered questions in this economic recovery is when will jobs return?\n",
    "Because without jobs, we're still going to have problems with consumer spending; we're still going to have problems with people not being able to pay their mortgages. I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what they think is going to happen in the future.\n",
    "Certainly temporary help could be a great leading indicator for the employment market overall, so it would be an interesting one to watch.\n",
    "So now for number two, a good indicator potentially for consumers, what do you have for the second one?\n",
    "Marriott International is one of my favorite travel bellwether stocks. They have hotels across the world under all different price points. And it shows both how consumers and business travelers are reacting to the economic environment. Is the \"stay-cation\" still ruling? Are business travelers and traveling salespeople finally hitting the road again?\n",
    "By looking at Marriott's results and seeing where the strength was and what travelers are out there--is it convention businesses? What kind of people are out there on the road? It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future.\n",
    "''')\n",
    "doc_u_1 = nlp(u'''Let's start out for number one the first company that you're looking forward to and what are you looking for in that report?\n",
    "The first one is Manpower, which is a huge, temporary staffing organization. In this report, I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market. One of the big unanswered questions in this economic recovery is when will jobs return?\n",
    "Because without jobs, we're still going to have problems with consumer spending; we're still going to have problems with people not being able to pay their mortgages. I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what they think is going to happen in the future.\n",
    "Certainly temporary help could be a great leading indicator for the employment market overall, so it would be an interesting one to watch.''')\n",
    "doc_u_2 = nlp(u'''So now for number two, a good indicator potentially for consumers, what do you have for the second one?\n",
    "Marriott International is one of my favorite travel bellwether stocks. They have hotels across the world under all different price points. And it shows both how consumers and business travelers are reacting to the economic environment. Is the \"stay-cation\" still ruling? Are business travelers and traveling salespeople finally hitting the road again? . By looking at Marriott's results and seeing where the strength was and what travelers are out there--is it convention businesses? What kind of people are out there on the road? It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future.\n",
    "''')\n",
    "doc_u_3 = nlp(u'''So now for number two, a good indicator potentially for consumers, what do you have for the second one?\n",
    "Marriott International is one of my favorite travel bellwether stocks. Marriott's results and seeing where the strength was and what travelers are out there--is it convention businesses? What kind of people are out there on the road? It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future.''')\n",
    "\n",
    "doc_u_4 = nlp(u'''I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what Manpower think is going to happen in the future. Certainly temporary help could be a great leading indicator for the employment market overall, so it would be an interesting one to watch. So now for number two, a good indicator potentially for consumers, what do you have for the second one ? Marriott International is one of my favorite travel bellwether stocks. ''')\n",
    "\n",
    "print(doc._.has_coref)\n",
    "print(doc._.coref_clusters)\n",
    "\n",
    "\n",
    "print(doc_u_1._.has_coref)\n",
    "print(doc_u_1._.coref_clusters)\n",
    "\n",
    "\n",
    "print(doc_u_2._.has_coref)\n",
    "print(doc_u_2._.coref_clusters)\n",
    "\n",
    "print(doc_u_3._.has_coref)\n",
    "print(doc_u_3._.coref_clusters)\n",
    "\n",
    "print(doc_u_4._.has_coref)\n",
    "print(doc_u_4._.coref_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import stanfordnlp\n",
    "st_nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos\")\n",
    "\n",
    "def get_lemma(doc):\n",
    "    doc = st_nlp(doc)\n",
    "    parsed_text = []#{k+'_'+'word':[], k+'_'+'lemma':[], k+'_'+'pos':[]}\n",
    "    for sent in doc.sentences:\n",
    "        tmp = []\n",
    "        for wrd in sent.words:\n",
    "            tmp.append(wrd.lemma)\n",
    "        parsed_text.append(tmp)\n",
    "    return parsed_text\n",
    "\n",
    "data = '''Let's start out for number one the first company that you're looking forward to and what are you looking for in that report?\n",
    "The first one is Manpower, which is a huge, temporary staffing organization. In this report, I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market. One of the big unanswered questions in this economic recovery is when will jobs return?\n",
    "Because without jobs, we're still going to have problems with consumer spending; we're still going to have problems with people not being able to pay their mortgages. I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what they think is going to happen in the future.\n",
    "Certainly temporary help could be a great leading indicator for the employment market overall, so it would be an interesting one to watch.\n",
    "So now for number two, a good indicator potentially for consumers, what do you have for the second one?\n",
    "Marriott International is one of my favorite travel bellwether stocks. They have hotels across the world under all different price points. And it shows both how consumers and business travelers are reacting to the economic environment. Is the \"stay-cation\" still ruling? Are business travelers and traveling salespeople finally hitting the road again?\n",
    "By looking at Marriott's results and seeing where the strength was and what travelers are out there--is it convention businesses? What kind of people are out there on the road? It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future.\n",
    "'''\n",
    "sents = get_lemma(data)\n",
    "#print(sents)\n",
    "\n",
    "#sents = \" \".join([\" \".join(i) for i in sents])\n",
    "#print(sents)\n",
    "#doc_r = nlp(sents)\n",
    "#print(doc_r._.has_coref)\n",
    "#print(doc._.coref_cluster)\n",
    "#print(doc_r._.coref_clusters)\n",
    "#print(doc_r._.coref_resolved)\n",
    "#print(doc_r.text_with_ws)\n",
    "#print(doc_r._.span_extensions.items())\n",
    "#print(doc_r._.token_extensions.items())\n",
    "#print([i for i in doc_r.noun_chunks])\n",
    "#print([i for i in doc_r.user_span_hooks])\n",
    "#print(dir(doc_r._))\n",
    "#print(dir(print(doc_r._.coref_clusters)))\n",
    "#print(doc_r.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Brown Coherence Toolkit: research software that models how sentences in a document relate to one another. Available under the GPL.\n",
    "* https://bitbucket.org/melsner/browncoherence/src/default/\n",
    "*  B&L   is   Elsner’s   “baseline   entitygrid” (command line option ’-n’), E&C is Elsner’s “extendedentity grid” (’-f’)\n",
    "* (AutomaticEvaluating Text Coherenceusing Discourse Relations) http://dspace.dtu.ac.in:8080/jspui/bitstream/repository/14260/1/Rajiv_Final_Thesis-1.pdf , https://www.ijcai.org/Proceedings/05/Papers/0505.pdf\n",
    "\n",
    "* Entity and syntax experiments for assessing coherence (https://github.com/karins/CoherenceFramework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-based Local Coherence Modeling\n",
    "* http://wing.comp.nus.edu.sg/~antho/P/P13/P13-1010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Coherence Modeling of Asynchronus Conversations: A Neural Entity Grid Approach.\n",
    "* https://github.com/taasnim/conv-coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn_coherence\n",
    "* https://github.com/datienguyen/cnn_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity-Graph\n",
    "! git clone https://github.com/nlpAThits/entity-graph.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster (Main Block). next two cell can be used for getting entity related text, if all dependencies fulfilled.\n",
    "\n",
    "\n",
    "* (i) whether both sentences are identical or not\n",
    "* (ii) whether one sentence includes another\n",
    "* (iii) whether both sentences share partial information\n",
    "* (iv) whether both sentences share the same subject of topic\n",
    "* (v) whether one sentence discusses any entity mentioned in another\n",
    "\n",
    "The priority of the discourse relations assignment can be concluded as follows:\n",
    "Identity> Subsumption> Elaboration> Overlap> Change of Topics > Description\n",
    "\n",
    "We then performed clustering algorithm to construct groupsof similar sentences. The algorithm is summarized as follows:\n",
    "* i) Assign the strongest relations determined by Discourse Parser.\n",
    "* ii) Suppose  each  sentence  is  a  centroid  of  its  own  cluster.  Identify  sentences  connected  to the  centroid  asIdentity (ID),  Subsumption (SUB), Elaboration (ELA)  and Overlap (OVE)relations1.  Sentences  with  these  connections  are  evaluated  as  having  similar content, and aggregated as one cluster (refer Figure 1(b)).\n",
    "* iii) Removesimilar  clustersby  retrievingcentroidsconnected  as Identity, Subsumptionor Elaboration. \n",
    "* iv) Merge  the  clusters  from  (iii)  to  minimize  the  occurrence  of  the  same  sentences  in multiple clusters (refer Figure 1(c)).\n",
    "* v) Iterate step (iii) and (iv) until the number of clusters is convergence.\n",
    "`\n",
    "\n",
    "#### Related Info\n",
    "\n",
    "\n",
    "Before going into further details, let's first give a definition of opinion. Text information can be broadly categorized into two main types: facts and opinions. Facts are objectiveexpressions about something. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, and feelings toward a subject or topic.\n",
    "\n",
    "Sentiment analysis, just as many other NLP problems, can be modeled as a classification problem where two sub-problems must be resolved:\n",
    "\n",
    "    Classifying a sentence as subjective or objective, known as subjectivity classification.\n",
    "    Classifying a sentence as expressing a positive, negative or neutral opinion, known as polarity classification.\n",
    "\n",
    "In an opinion, the entity the text talks about can be an object, its components, its aspects, its attributes, or its features. It could also be a product, a service, an individual, an organization, an event, or a topic. As an example, take a look at the opinion below:\n",
    "\n",
    "\"The battery life of this camera is too short.\"\n",
    "\n",
    "A negative opinion is expressed about a feature (battery life) of an entity (camera).\n",
    "\n",
    "Direct vs Comparative Opinions\n",
    "\n",
    "There are two kinds of opinions: direct and comparative. Direct opinions give an opinion about a entity directly, for example:\n",
    "\n",
    "\"The picture quality of camera A is poor.\"\n",
    "\n",
    "This direct opinion states a negative opinion about camera A.\n",
    "\n",
    "In comparative opinions, the opinion is expressed by comparing an entity with another, for example:\n",
    "\n",
    "“The picture quality of camera A is better than that of camera B.”\n",
    "\n",
    "Usually, comparative opinions express similarities or differences between two or more entities using a comparative or superlative form of an adjective or adverb. In the previous example, there's a positive opinion about camera A and, conversely, a negative opinion about camera B.\n",
    "\n",
    "Explicit vs Implicit Opinions\n",
    "\n",
    "An explicit opinion on a subject is an opinion explicitly expressed in a subjective sentence. The following sentence expresses an explicit positive opinion:\n",
    "\n",
    "“The voice quality of this phone is amazing.”\n",
    "\n",
    "An implicit opinion on a subject is an opinion implied in an objective sentence. The following sentence expresses an implicit negative opinion:\n",
    "\n",
    "“The earphone broke in two days.”\n",
    "\n",
    "Within implicit opinions we could include metaphors that may be the most difficult type of opinions to analyze as they include a lot of semantic information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/mluser/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/mluser/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/mluser/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/mluser/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "#from ordered_set import OrderedSet\n",
    "import requests\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import stanfordnlp\n",
    "st_nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos\")\n",
    "\n",
    "# Training -- https://github.com/huggingface/neuralcoref/blob/master/neuralcoref/train/training.md\n",
    "import neuralcoref\n",
    "neu_nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(neu_nlp)\n",
    "# Here are three ways we can add the conversion dictionary\n",
    "#neu_nlp.remove_pipe(\"neuralcoref\")\n",
    "#neuralcoref.add_to_pipe(neu_nlp, conv_dict={'Manpower': ['organisation', 'company','corporate']})\n",
    "\n",
    "def get_lemma(text):\n",
    "    '''\n",
    "    get lemmatized form of words from text.\n",
    "    '''\n",
    "    doc = st_nlp(text)\n",
    "    parsed_text = []\n",
    "    for sent in doc.sentences:\n",
    "        tmp = []\n",
    "        for wrd in sent.words:\n",
    "            tmp.append(wrd.lemma)\n",
    "        parsed_text.append(tmp)\n",
    "    \n",
    "    return parsed_text\n",
    "\n",
    "def get_pos_tags(text):\n",
    "    #text = \"I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market.\"\n",
    "    text1 = SpaceTokenizer().tokenize(text)\n",
    "    return nltk.pos_tag(text1)\n",
    "\n",
    "def replace_pronouns(text):\n",
    "    '''\n",
    "    use coreference resolution for replacing pronouns with nouns.\n",
    "    '''\n",
    "    doc = neu_nlp(text)\n",
    "    \n",
    "    return doc._.coref_resolved\n",
    "\n",
    "def split_sentence(spacy_obj, text):\n",
    "    '''\n",
    "    splits review into a list of sentences using spacy's sentence parser\n",
    "    '''\n",
    "    sents = spacy_obj(text)\n",
    "    bag_sentence = []\n",
    "    start = 0\n",
    "    for token in sents:\n",
    "        if token.sent_start:\n",
    "            bag_sentence.append(sents[start:(token.i-1)])\n",
    "            start = token.i\n",
    "        if token.i == len(sents)-1:\n",
    "            bag_sentence.append(sents[start:(token.i+1)])\n",
    "    \n",
    "    return bag_sentence\n",
    "\n",
    "def remove_special_char(sentence):\n",
    "    '''\n",
    "    Remove special characters using regex\n",
    "    '''\n",
    "    \n",
    "    return re.sub(r\"[^a-zA-Z0-9.',:;?]+\", ' ', sentence)\n",
    "\n",
    "def get_all_entity(spacy_obj , x):\n",
    "    '''\n",
    "    using spacy object get all entities.\n",
    "    '''\n",
    "    doc = spacy_obj(x)\n",
    "    \n",
    "    return [(ent.text,ent.label_) for ent in doc.ents if ent.label_ in ['ORG','GPE','LOC']] #,'PERSON'\n",
    "\n",
    "#sents = get_lemma(doc)\n",
    "#sents = \" \".join([\" \".join(i) for i in sents])\n",
    "#doc = neu_nlp(u'My sister has a dog. She loves him')\n",
    "#print(doc._.has_coref)\n",
    "#print(doc._.coref_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that report\n",
      "[\"Let's start out for number one the first company that you're looking forward to and what are you looking for in that report?\", \"In this report, I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market.\"]\n",
      "\n",
      "the labor market\n",
      "[\"In this report, I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market.\", \"I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what they think is going to happen in the future.\"]\n",
      "\n",
      "people\n",
      "[\"Because without jobs, we're still going to have problems with consumer spending; we're still going to have problems with people not being able to pay their mortgages.\", 'So unlike if Wal-Mart does well, it means people are still buying those everyday things that they always need.', 'If Target earnings start to do really well, it could show that people are feeling a little bit more confident and that could bode well for their company.']\n",
      "\n",
      "Manpower\n",
      "[\"I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what they think is going to happen in the future.\"]\n",
      "\n",
      "Marriott International\n",
      "['So now for number two, a good indicator potentially for consumers, what do you have for the second one?', 'Marriott International is one of my favorite travel bellwether stocks.', 'They have hotels across the world under all different price points.']\n",
      "\n",
      "the road\n",
      "['Are business travelers and traveling salespeople finally hitting the road again?', 'What kind of people are out there on the road?']\n",
      "\n",
      "the strength\n",
      "[\"By looking at Marriott's results and seeing where the strength was and what travelers are out there--is it convention businesses?\", 'What kind of people are out there on the road?', 'It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future.']\n",
      "\n",
      "us\n",
      "['It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future.']\n",
      "\n",
      "Target\n",
      "[\"There's going to be a lot of interesting retail earnings, which come a little bit later in earning season, but Target, I think, is going to be one to watch.\", 'They came out this week and said that earnings for the first quarter are going to be a little bit better than they initially expected.  ', 'Target has a lot of discretionary goods that they sell.', 'If Target earnings start to do really well, it could show that people are feeling a little bit more confident and that could bode well for their company.']\n",
      "\n",
      "They\n",
      "[\"There's going to be a lot of interesting retail earnings, which come a little bit later in earning season, but Target, I think, is going to be one to watch.\", 'They came out this week and said that earnings for the first quarter are going to be a little bit better than they initially expected.  ']\n",
      "\n",
      "the first quarter\n",
      "['They came out this week and said that earnings for the first quarter are going to be a little bit better than they initially expected.  ', \"HP has really become the behemoth when it comes to selling computers, both to consumers and businesses, but the thing that's going to be really interesting this quarter is how enterprise IT spending is holding up.\"]\n",
      "\n",
      "Wal-Mart\n",
      "['So unlike if Wal-Mart does well, it means people are still buying those everyday things that they always need.']\n",
      "\n",
      "Target earnings\n",
      "['If Target earnings start to do really well, it could show that people are feeling a little bit more confident and that could bode well for their company.']\n",
      "\n",
      "HP\n",
      "['If Target earnings start to do really well, it could show that people are feeling a little bit more confident and that could bode well for their company.', \"HP has really become the behemoth when it comes to selling computers, both to consumers and businesses, but the thing that's going to be really interesting this quarter is how enterprise IT spending is holding up.\"]\n",
      "\n",
      "their\n",
      "['Are businesses investing in their future through servers and through desktops and through laptops?  ', \"If they're really spending a lot of money out there, it could be a sign that they're looking to add to their workforce or they want to equip their workforce with even better equipment.\"]\n",
      "\n",
      "companies\n",
      "['It shows that companies are optimistic about the future, which could make us optimistic about the future of those companies.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''Let's start out for number one the first company that you're looking forward to and what are you looking for in that report?\n",
    "The first one is Manpower, which is a huge, temporary staffing organization. In this report, I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market. One of the big unanswered questions in this economic recovery is when will jobs return?\n",
    "Because without jobs, we're still going to have problems with consumer spending; we're still going to have problems with people not being able to pay their mortgages. I think that Manpower usually has some pretty good insights as to what's happening in the labor market and what they think is going to happen in the future.\n",
    "Certainly temporary help could be a great leading indicator for the employment market overall, so it would be an interesting one to watch.\n",
    "So now for number two, a good indicator potentially for consumers, what do you have for the second one?\n",
    "Marriott International is one of my favorite travel bellwether stocks. They have hotels across the world under all different price points. And it shows both how consumers and business travelers are reacting to the economic environment. Is the \"stay-cation\" still ruling? Are business travelers and traveling salespeople finally hitting the road again?\n",
    "By looking at Marriott's results and seeing where the strength was and what travelers are out there--is it convention businesses? What kind of people are out there on the road? It can tell us a lot about where we are in the economic recovery and what we can look forward to in the future. There's going to be a lot of interesting retail earnings, which come a little bit later in earning season, but Target, I think, is going to be one to watch. They came out this week and said that earnings for the first quarter are going to be a little bit better than they initially expected.\n",
    "\n",
    "Target has a lot of discretionary goods that they sell. So unlike if Wal-Mart does well, it means people are still buying those everyday things that they always need. I'm less concerned now about the recession that people are just cutting out all spending. What we want to see is, are people out there and buying more video games or buying more home decor items--things that aren't necessary, but that people would really like and have been holding off for a while.\n",
    "\n",
    "If Target earnings start to do really well, it could show that people are feeling a little bit more confident and that could bode well for their company. HP has really become the behemoth when it comes to selling computers, both to consumers and businesses, but the thing that's going to be really interesting this quarter is how enterprise IT spending is holding up. Are businesses investing in their future through servers and through desktops and through laptops?\n",
    "\n",
    "If they're really spending a lot of money out there, it could be a sign that they're looking to add to their workforce or they want to equip their workforce with even better equipment. It shows that companies are optimistic about the future, which could make us optimistic about the future of those companies.'''\n",
    "\n",
    "text1 = '''Good morning. My name is Amanda, and I will be your conference operator. At this time, I would like to welcome everyone to ADP First Quarter 2011 Earnings Webcast. [Operator Instructions] I will now turn the conference over to Ms. Elena Charles, Vice President, Investor Relations. Please go ahead. Thank you. Good morning. I'm here today with Gary Butler, ADP's President and CEO; and Chris Reidy, ADP's Chief Financial Officer. Thank you for joining us this morning for our First Quarter Fiscal 2011 Earnings Call and Webcast. Our slide presentation for today's call and webcast is available for you to print from the Investor Relations homepage of our website at adp.com.\n",
    "\n",
    "As a reminder, the quarterly history of revenue and pretax earnings for our reportable segments has been posted to the IR section of our website. These schedules have been updated to include the first quarter of fiscal 2011, and all prior periods have been updated to reflect 2011 budgeted foreign exchange rates.\n",
    "\n",
    "During today's conference call, we will make some forward-looking statements that refer to future events and as such, involve some risks, and these are discussed on Page 2 of the slide presentation and in our periodic filings with the SEC. \n",
    "Highlights of Greenbrier's second fiscal quarter included solid railcar order activity and increased railcar deliveries and revenue generation, but our financial performance in the quarter was obviously disappointing. Our 2019 plan indemnified our second quarter as the least profitable of the year, due in part to expected manufacturing inefficiencies from production line changeovers and building of a fleet of cars for syndication.\n",
    "\n",
    "However, we also encountered three distinct headwinds mentioned in our press release that impacted profitability in the quarter, specifically challenges in manufacturing at our Romanian and Gunderson facilities and facility closure costs and railcar repair operations. These factors caused second quarter earnings per share to fall short of our goals. Despite these challenges, during the quarter, our projected deliveries and revenue for the year remain on target. However, we now estimate fiscal 2019 earnings per share in the range of $3.60 to $3.80, excluding the railcar contract loss accruals and facility closure costs incurred during the quarter.\n",
    "\n",
    "Adrian will discuss our second quarter financial results and full-year financial guidance in more detail. While lower than our previous outlook, we are confident in Greenbrier's ability to perform within this range. We have identified the core issues creating the negative financial performance in our Romanian facilities, in particular in our Gunderson operations in our railcar repair network. By far, the Romanian operations were the most significant and we believe we have a new handle on that with new management under the leadership of longtime Greenbrier officer, William Glenn, who used to run our European operations and has recently rejoined the Company. We are also making rapid progress in addressing other performance issues that impacted the quarter.'''\n",
    "\n",
    "text2 = '''Let me begin today’s call with some opening remarks about our first quarter results. I’ll then turn the call over to Chris Reidy who will take you through the detailed results, and then I’ll return a little bit later to give you and updated forecast for fiscal 2010. Before we take your questions I will provide some concluding remarks.\n",
    "\n",
    "To begin, ADP’s first quarter results were against very tough comparisons a year ago when we posted 9.5% revenue growth, pre-tax margin expansion of 100 basis points and 20% EPS growth. As you recall it was toward the end of the first quarter in fiscal ’09 that the financial market volatility led to the most difficult economy in decades. Considering the cumulative economic impact on ADP’s business metrics in the quarter, which include lower new business sales, lower client retention, lower client fund balance, fewer number of payees and continued dealership closings, I am encouraged by what we achieved this quarter.\n",
    "\n",
    "Revenues for the first quarter declined 4% year over year but were slightly ahead of our expectations. Foreign exchange rates gave us a revenue benefit of 1% in last year ’09 first quarter but had been working against us throughout fiscal 2010 in the first quarter. The current quarter’s revenues were negatively impacted about two percentage points from unfavorable exchange rates. However, I am pleased that ADP posted positive growth in both pre-tax and net earnings of 1% and 2% respectively. Earnings per share from continuing operations grew 4% on fewer shares outstanding.\n",
    "\n",
    "New business sales declined 2% in the first quarter which is an improvement over the decline posted through fiscal 2009. These results were ahead of our expectations but were mixed by business unit within employer services. Sales in our National Accounts Division declined year over year as the sales cycle for larger companies remained challenging with continued delays in outsourcing decisions. Sales to mid-side companies in our Major Accounts Division grew compared with last year’s first fiscal quarter.\n",
    "Welcome to the RPM International Inc. investor call for our fiscal 2019 third quarter ended February 28, 2019. On the call with me today are Rusty Gordon, RPM's vice president and chief financial officer; and Kristine Schulze, our senior director of financial reporting.\n",
    "\n",
    "I'll begin by providing broad perspective on our third-quarter results, after which Kristine will run through our numbers in more detail. She'll be followed by Rusty Gordon, who will provide a progress update on our MAP to Growth operating improvement plan and share our outlook for the balance of the year. Note that during our comments, we will walk through some slides that we have posted on the Investor Information section of our website, which can be found at www.rpminc.com, which illustrate progress on our MAP to Growth operating improvement plan. After this, we'll take your questions.\n",
    "'''\n",
    "text4 = ''' India's captain Virat Kohli, left, and Rohit Sharma run between the wickets to score during the Cricket World Cup warm up match between Bangladesh and India at Sophia Gardens in Cardiff. Photo: AP/PTI\n",
    "In Match 40 of ICC Cricket World Cup 2019 (CWC), India cricket team defeated Bangladesh by 28 runs and throw to the semifinals as Bumrah once again displayed a brilliant death bowling skills at Edgbaston cricket ground in Birmingham. \n",
    "Earlier, Rohit Sharma continued his dream run with a record equalling fourth century but the Indian innings once again lacked the final flourish, managing a par score of 314 for nine against Bangladesh cricket team in their penultimate World Cup league. \n",
    "MS Dhoni (35 off 33 balls) once again failed to get going as India managed only 63 runs in the last 10 overs after Rohit's 26th ODI century and Rishabh Pant's (48 off 40 balls) flamboyance had taken India to 251 for 4. \n",
    "Dhoni looked ill at ease against Mustafizur Rahaman's (5/59 in 10 overs) brilliant variations and Shakib Al Hasan's (1/41 in 10 overs) accuracy. \n",
    "The former India skipper refused two singles in the final over and was out off the third delivery as India lost momentum towards end of the innings.'''\n",
    "\n",
    "text5 = '''A sustained rally in select PSU bank stocks took the country’s biggest lender State Bank of India (SBI) to its highest level of Rs 370 on Dalal Street in morning trade on Thursday. At these heights, the stock appears to have broken out of its multi-year consolidation zone placed Rs 350 and Rs 150 levels.\n",
    "Analysts said the primary reason that can be attributed to this rally is hopes of government making fresh capital provisions for PSU banks in the forthcoming Union Budget. . “If it sustains above Rs 350 level, then there will be a higher possibility of the stock outperforming its peers with a possible target of Rs 487 based on our long-term trend studies. Positional traders can continue with a stop loss below Rs 350 on closing basis, below which the stock may once again test its strong support placed around Rs 330 on the long-term charts, where best buying opportunity should arise,” he said.\n",
    "Overall, PSU lenders Syndicate Bank and Allahabad Bank have rallie ..'''\n",
    "\n",
    "text6 = '''The Economic Survey 2019 on Thursday projected a rosy picture for the economy, saying it will grow at a healthy 7 per cent this financial year, riding a stable macro environment.\n",
    "\n",
    "The survey forecast a rebound in investment cycle in FY20. “The investment rate seems to have bottomed out,” it suggested, adding that “green shoots of investment activity seem to be taking hold.”\n",
    "\n",
    "The growth estimates should be music to the ears of investors, as the economy is showing clear signs of s ..\n",
    "\n",
    "During the last five years, India’s economy has performed well. By opening up several pathways for trickle-down, the government has ensured that the benefits of growth and macroeconomic stability reach the bottom of the pyramid.\n",
    "\n",
    "To achieve the objective of becoming a US$5 trillion economy by 2024-25, India needs to sustain a real GDP growth rate of 8%. International experience, especially from high-growth East Asian economies, suggests that such growth can only be sustained by a “virtu ..\n",
    "'''\n",
    "text7 = '''ADP’s first quarter results were against very tough comparisons a year ago when we posted 9.5% revenue growth, pre-tax margin expansion of 100 basis points and 20% EPS growth.'''\n",
    "text8 = '''numbers are the most disturbing thing in this world'''\n",
    "text9 ='''Union Finance Minister Nirmala Sitharaman in Budget 2019 proposed many changes to the income tax rules governing investment and withdrawal in retirement pension scheme NPS, though some of them were approved by the Union Cabinet last year. The finance minister increased the income tax exemption limit on withdrawal from NPS while also announced some additional tax benefits for central government employees who contribute towards the retirement scheme. Tax experts have welcomed the higher exemption limit on NPS but said that some the proposed tax benefits for central government employees should be also extended to other sections of the society. These amendments will take effect from assessment year 2020-21. Some of the changes proposed to NPS are based on 7th Pay Commission. \n",
    "The government had made certain announcements in December 2018 in relation to NPS, wherein it raised its contribution to Tier I account for its employees from 10% to 14% and also announced that the tax benefit to such employees in respect of their own contribution to Tier II account will be available u/s 80C with a lock-in period of 3 years. The Finance Minister has proposed the changes in the Budget to implement the announcements. Further, the exemption of lump sum payment from NPS upon retirement is proposed to be raised from 40% to 60%, as also announced earlier.'''\n",
    "\n",
    "def get_entity_rel_sents_using_coref(neu_nlp, doc_in):\n",
    "    '''\n",
    "    input : spacy object & document.\n",
    "    \n",
    "    DESC : get entity and their related texts by using only co-reference resolution.\n",
    "    \n",
    "    return dictionary having entity and related texts.\n",
    "    '''\n",
    "    doc = neu_nlp(doc_in)\n",
    "    cluster_sents_dict = OrderedDict()\n",
    "    if doc._.has_coref:\n",
    "        sent_posin_info = [(se.start,se.end,se.ent_id,se.ent_id_,se.text) for se in doc.sents]\n",
    "        #'''\n",
    "        for each_cluster_ind, val in enumerate(doc._.coref_clusters):\n",
    "            clust = doc._.coref_clusters[each_cluster_ind]\n",
    "            for ind, val_m in enumerate(clust.mentions):\n",
    "                start_ind = clust.mentions[ind].start\n",
    "                end_ind = clust.mentions[ind].end\n",
    "                tmp_exists = [each_tup[4] for each_tup in sent_posin_info if start_ind >= each_tup[0] and start_ind <=each_tup[1]]\n",
    "                if len(tmp_exists) >= 1:\n",
    "                    cluster_sents_dict.setdefault(clust.mentions[0].text.strip(), []).extend(tmp_exists)\n",
    "        \n",
    "        #'''\n",
    "        '''\n",
    "        response = {}\n",
    "        mentions = [{'start':    mention.start,\n",
    "                    'end':      mention.end,\n",
    "                    'text':     mention.text,\n",
    "                    'resolved': cluster.main.text\n",
    "                    }\n",
    "                    for cluster in doc._.coref_clusters\n",
    "                    for mention in cluster.mentions]\n",
    "        clusters = list(list(span.text for span in cluster)\n",
    "                    for cluster in doc._.coref_clusters)\n",
    "        resolved = doc._.coref_resolved\n",
    "        response['mentions'] = mentions\n",
    "        response['clusters'] = clusters\n",
    "        response['resolved'] = resolved\n",
    "        print(\"*****\")\n",
    "        print(doc._.coref_clusters)\n",
    "        print(\"######\")\n",
    "        print(clusters)\n",
    "        #print(response['mentions'])\n",
    "            \n",
    "        for each_mention in response['mentions']:\n",
    "            tmp_exists = [each_tup[4] for each_tup in sent_posin_info if each_mention['start'] >= each_tup[0] and each_mention['end'] <=each_tup[1]]\n",
    "            if len(tmp_exists) >= 1:\n",
    "                cluster_sents_dict.setdefault(each_mention['resolved'].strip(), []).extend(tmp_exists)\n",
    "        '''\n",
    "        for key in cluster_sents_dict.keys():\n",
    "            cluster_sents_dict[key] = list(OrderedDict.fromkeys(cluster_sents_dict[key]))\n",
    "        \n",
    "\n",
    "    return cluster_sents_dict\n",
    "\n",
    "def get_entity_rel_sents(spacy_obj , doc):\n",
    "    '''\n",
    "    input : spacy object & document.\n",
    "    \n",
    "    DESC:\n",
    "    1. convert all implicit entity to explicit entity using co-reference resolution.\n",
    "    2. get central entity sentence\n",
    "    2.1.    # start from here and go until you reach new entity with new discussion. not comparision.\n",
    "    2.2.    # use co-reference resolution. (if 1. used , then don't use 2.2)\n",
    "    2.3.    # use discourse parser for checking only Identy , Subsumption , Elaboration.\n",
    "    2.4.    # use coherence modelling\n",
    "    2.5.    # use text entailment\n",
    "    \n",
    "    return dictionary having entity and related texts.\n",
    "    '''\n",
    "    base_sents_dict = OrderedDict()\n",
    "    all_sents = []\n",
    "    base_sents = []\n",
    "    cluster_sents = OrderedDict()\n",
    "    \n",
    "    # sentence segmentation. and entity identification.\n",
    "    #doc = replace_pronouns(doc)\n",
    "    data_doc = spacy_obj(doc)\n",
    "    for ind, sent in enumerate(data_doc.sents):\n",
    "        # only get sentences with length more than 2.\n",
    "        if len(sent.text) > 2:\n",
    "            text = sent.text.strip(\"\\n\")\n",
    "            all_sents.append(text)\n",
    "            ent_list = get_all_entity(spacy_obj , text)\n",
    "            print(\"ent_list : \",ent_list)\n",
    "            #print(\"text : \",text)\n",
    "            for i in ent_list:\n",
    "                # entity with multi words or possesive form of word, get only first word as entity.\n",
    "                #if len(i[0].split(\" \")) > 1:\n",
    "                #    entity = i[0].split(\" \")[0]\n",
    "                #elif len(i[0].split(\"'\")) > 1:\n",
    "                #    entity = i[0].split(\"'\")[0]\n",
    "                #else:\n",
    "                #    entity = i[0]\n",
    "                entity = i[0]\n",
    "                #print(\"### : \",ind,text,get_pos_tags(text))\n",
    "                base_sents_dict.setdefault(entity, []).append((ind,text))\n",
    "    \n",
    "    # filter only Targeted entity.\n",
    "    #base_sents_dict\n",
    "    print(\"filter with Targeted Entity.\")\n",
    "    print(base_sents_dict)\n",
    "    \n",
    "    # return all sents, if no entity found.\n",
    "    if len(base_sents_dict.keys()) < 1:\n",
    "        return {'all_sents':all_sents,'base_sents':None,'cluster_sents':None}\n",
    "\n",
    "    # find central/base sentence where event conversation starts.\n",
    "    for key in base_sents_dict.keys():\n",
    "        # check for comparision statement !!.\n",
    "        base_sents.append((base_sents_dict.get(key)[0][0], key, base_sents_dict.get(key)))\n",
    "    \n",
    "    # discourse parser for relation extraction.\n",
    "    if use_discourse == True:\n",
    "        dc_output = 500\n",
    "        if os.path.exists('StageDP/data/test5/tmp'):\n",
    "            os.remove('StageDP/data/test5/tmp')\n",
    "        if os.path.exists('StageDP/data/test5/tmp.edus'):\n",
    "            os.remove('StageDP/data/test5/tmp.edus')\n",
    "        with open('StageDP/data/test5/tmp','w') as f, open('StageDP/data/test5/tmp.edus','w') as fa:\n",
    "            for each_sent in all_sents:\n",
    "                f.write(each_sent+\"\\n\")\n",
    "                fa.write(each_sent+\"\\n\")\n",
    "\n",
    "        url = 'http://localhost:5000/todo/api/v1.0/tasks'\n",
    "        data = '''{\"title\":\"get parsed dependency tree\"}'''\n",
    "        response = requests.post(url, data=data)\n",
    "        #print(response)\n",
    "        resp = response.json()\n",
    "        dc_output = resp['task']['title']\n",
    "        if dc_output != 500:\n",
    "            print(dc_output)\n",
    "    \n",
    "    # iterate over each central/base entity-sentence. and cluster related sentences.\n",
    "    for ind, bs in enumerate(base_sents):\n",
    "        #print(ind,\"***\",bs)\n",
    "        if ind+1 <= len(base_sents)-1:\n",
    "            tmp_sents = all_sents[bs[0]:base_sents[ind+1][0]]\n",
    "            tmp_sents_range = (bs[0],base_sents[ind+1][0])\n",
    "        else:\n",
    "            tmp_sents = all_sents[bs[0]:len(all_sents)]\n",
    "            tmp_sents_range = (bs[0],len(all_sents))\n",
    "        #print(\"===tmp_sents===\")\n",
    "        #print([i for i in range(bs[0],base_sents[ind+1][0])])\n",
    "        #print(tmp_sents)\n",
    "        #sents = get_lemma(doc)\n",
    "        #sents = \" \".join([\" \".join(i) for i in sents])\n",
    "        \n",
    "        # check for coreference. filter out unwanted sentences. keep sentences where entity used explicitly.\n",
    "        tmp_e_sents = []\n",
    "        if use_coref != True:\n",
    "            tmp_e_sents = tmp_sents\n",
    "        else:\n",
    "            for _ind, t_sent in enumerate(tmp_sents):\n",
    "                if _ind == 0:\n",
    "                    tmp_e_sents.append(t_sent)\n",
    "                    continue\n",
    "                tmp_doc = neu_nlp(\" \".join(tmp_e_sents[0:1] + [t_sent]))\n",
    "                #print(\"#####\")\n",
    "                #print(tmp_e_sents ,\"****\", [t_sent])\n",
    "                #print(tmp_doc._.has_coref)\n",
    "                if tmp_doc._.has_coref:\n",
    "                    tmp_e_sents.append(t_sent)\n",
    "\n",
    "                #print(tmp_doc._.has_coref)\n",
    "                #print(tmp_doc._.coref_clusters)\n",
    "            \n",
    "        # check for relation. filter out unwanted sentences.\n",
    "        tmp_r_sents = []\n",
    "        if use_discourse == True:\n",
    "            if dc_output != 500:\n",
    "                print(\"===relation===\")\n",
    "                for ind_ in range(tmp_sents_range):\n",
    "                    print([i for i in dc_output if i[0][0] == ind_ or i[0][1] == ind_])\n",
    "                    rel_neuc_filter = set([i[0][0] for i in dc_output if i[0][0] == ind_ and i[1]=='Nucleus'])\n",
    "                    print(rel_neuc_filter)\n",
    "                    print([all_sents[i] for i in rel_neuc_filter])\n",
    "                    tmp_r_sents.extend([all_sents[i] for i in rel_neuc_filter])\n",
    "                print(\"=====\")\n",
    "                print(tmp_r_sents)\n",
    "\n",
    "            else:\n",
    "                print(\"some issue in discourse parser. can't use relation for filter.\")\n",
    "            \n",
    "        # check for coherence. filter\n",
    "            \n",
    "        # sentence ordering using coherence.\n",
    "            \n",
    "        # entailment finding\n",
    "            \n",
    "        # add all check-ons ,if not exists.\n",
    "        for f_sents in tmp_r_sents:\n",
    "            if not f_sents in tmp_e_sents:\n",
    "                tmp_e_sents.append(f_sents)\n",
    "        \n",
    "        # subjectiveness\n",
    "        tmp_s_sents = []\n",
    "        if use_subj == True:\n",
    "            for s_sents in tmp_e_sents:\n",
    "                tmp_s_sents.append((s_sents, round(TextBlob(s_sents).sentiment[1],2)))\n",
    "            \n",
    "        # embedd to cluster.\n",
    "        cluster_sents.setdefault(bs[1], []).append(tmp_s_sents)\n",
    "\n",
    "    return {'all_sents':all_sents,'base_sents':base_sents,'cluster_sents':cluster_sents}\n",
    "\n",
    "\n",
    "# Global Variables\n",
    "use_coref = False\n",
    "use_subj = True\n",
    "use_discourse = False\n",
    "text = text.replace(\"\\n\",\" \")\n",
    "#text = get_lemma(text)\n",
    "#text = \" \".join([\" \".join(i) for i in text])\n",
    "#print(text)\n",
    "'''\n",
    "entity_rel_sents = get_entity_rel_sents(neu_nlp , text)\n",
    "all_sents = entity_rel_sents['all_sents'] \n",
    "base_sents = entity_rel_sents['base_sents'] \n",
    "cluster_sents = entity_rel_sents['cluster_sents']\n",
    "if cluster_sents != None:\n",
    "    for key in cluster_sents.keys():\n",
    "        print(key)\n",
    "        print(get_lemma(key))\n",
    "        print(cluster_sents.get(key))\n",
    "        print()\n",
    "else:\n",
    "    print(all_sents)\n",
    "'''\n",
    "#'''\n",
    "cluster_sents = get_entity_rel_sents_using_coref(neu_nlp, text)\n",
    "for key in cluster_sents.keys():\n",
    "    print(key)\n",
    "    #print(get_lemma(key))\n",
    "    print(cluster_sents.get(key))\n",
    "    print()\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from textblob import TextBlob\n",
    "\n",
    "#TextBlob(\"not a very great calculation\").sentiment\n",
    "#TextBlob(\"So now for number two, a good indicator potentially for consumers, what do you have for the second one?\").sentiment\n",
    "TextBlob(\"Are business travelers and traveling salespeople finally hitting the road again ?\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['salespeople'], ['still', 'finally', 'again'], 1, 3)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "'''\n",
    "One reason researchers might care about parts of speech is that they can help reveal attributes of written text. For example, research has shown the presence of adjectives and adverbs is usually a good indicator of text subjectivity. In other words, statements that use adjectives like “interesting,” “problematic” and “awesome” might be more likely to convey a subjective point of view than statements that do not include those adjectives. Adjectives and adverbs might also be especially likely to convey tone: Some indicate more positive qualities, while others indicate negative qualities.\n",
    "\n",
    "Adjective analysis using off-the-shelf packages and dictionaries can provide an accessible and inexpensive way to explore sentiment and subjectivity in texts, without the need to obtain a large labeled training set. However, researchers should be cautious when drawing inferences from the tools described here. There’s no guarantee that adjectives provide a valid measure of important concepts like subjectivity across diverse contexts, and the link between particular adjectives and sentiment may not generalize across different domains. It’s always necessary to validate a given model — either by using human coders, or by supplementing the analysis with other measures of subjectivity — before drawing strong conclusions about what text tells us.\n",
    "'''\n",
    "def textblob_adj(text):\n",
    "    blobed = TextBlob(text)\n",
    "    counts = Counter(tag for word,tag in blobed.tags)\n",
    "    adj_list = []\n",
    "    adv_list = []\n",
    "    adj_tag_list = ['JJ','JJR','JJS']\n",
    "    adv_tag_list = ['RB','RBR','RBS']\n",
    "    for (a, b) in blobed.tags:\n",
    "        if b in adj_tag_list:\n",
    "           adj_list.append(a)\n",
    "        elif b in adv_tag_list:\n",
    "           adv_list.append(a)\n",
    "        else:\n",
    "            pass\n",
    "    return adj_list, adv_list, counts['JJ']+counts['JJR']+counts['JJS'], counts['RB']+counts['RBR']+counts['RBS']\n",
    "\n",
    "print(textblob_adj(\"Is the stay-cation still ruling? Are business travelers and traveling salespeople finally hitting the road again?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The singular of  apples  is  apple\n",
      "The singular of  sheep's  is  sheep'\n",
      "The singular of  oranges  is  orange\n",
      "The singular of  cats  is  cat\n",
      "The singular of  people's  is  people'\n",
      "The singular of  dice  is  die\n",
      "The singular of  pence  is  pence\n",
      "[(\"I'm\", 'NNP'), ('less', 'RBR'), ('interested', 'JJ'), ('in', 'IN'), ('actually', 'RB'), (\"Manpower's\", 'NNP'), ('earnings,', 'NN'), ('and', 'CC'), ('more', 'RBR'), ('interested', 'JJ'), ('in', 'IN'), ('what', 'WP'), ('they', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('say', 'VB'), ('about', 'IN'), ('the', 'DT'), ('state', 'NN'), ('of', 'IN'), ('the', 'DT'), ('labor', 'NN'), ('market.', 'NN')]\n",
      "[(Manpower, 'PERSON', 380)]\n",
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'to_json', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n",
      "[I, Manpower's earnings, what, they, the state, the labor market]\n",
      "I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market.\n",
      "I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market.\n",
      "owner-possesion tuple for entity transition..\n",
      "[(Manpower, earnings)]\n",
      "['practice', 'practice', 'practice', 'practice', \"'s\", 'practice']\n"
     ]
    }
   ],
   "source": [
    "#! pip install inflect\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "words = [\"apples\", \"sheep's\", \"oranges\", \"cats\", \"people's\", \"dice\", \"pence\"]\n",
    "\n",
    "for word in words:\n",
    "    print(\"The singular of \", word, \" is \", p.singular_noun(word))\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "text = \"I'm less interested in actually Manpower's earnings, and more interested in what they have to say about the state of the labor market.\"\n",
    "text1 = SpaceTokenizer().tokenize(text)\n",
    "print(nltk.pos_tag(text1))\n",
    "\n",
    "nlp_obama = neu_nlp(text)\n",
    "print([(i, i.label_, i.label) for i in nlp_obama.ents])\n",
    "print(dir(nlp_obama))\n",
    "print(list(nlp_obama.noun_chunks))\n",
    "print(nlp_obama.text)\n",
    "print(nlp_obama.text_with_ws)\n",
    "\n",
    "pos_tags = [(i, i.tag_) for i in nlp_obama]\n",
    "print(\"owner-possesion tuple for entity transition..\")\n",
    "print([(i[0].nbor(-1), i[0].nbor(+1)) for i in pos_tags if i[1] == \"POS\"])\n",
    "\n",
    "\n",
    "practice = \"practice practiced practicing practice's practices\" \n",
    "nlp_practice = neu_nlp(practice) \n",
    "print([word.lemma_ for word in nlp_practice])\n",
    "\n",
    "# phrase matcher\n",
    "# https://stackabuse.com/python-for-nlp-vocabulary-and-phrase-matching-with-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Linking with Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! wget https://homes.cs.washington.edu/~lzilles/code/neco.tar.gz\n",
    "#! tar -xvzf neco.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/gregdurrett/berkeley-entity.git\n",
    "#import os\n",
    "#os.chdir('berkeley_entity')\n",
    "#! wget http://nlp.cs.berkeley.edu/downloads/berkeley-entity-models.tgz\n",
    "#! tar -xvzf berkeley-entity-models.tgz\n",
    "#!wget http://www.cs.utexas.edu/~gdurrett/data/gender.data.tgz\n",
    "#!tar -xvzf gender.data.tgz\n",
    "#!sh pull-datasets.sh\n",
    "#! sh run_test.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/nitishgupta/neural-el.git\n",
    "# resourse file size is 14GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
